{"cells":[{"cell_type":"markdown","metadata":{},"source":["SEBASTIAN BELALCAZAR MOSQUERA"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":219,"status":"ok","timestamp":1715387878490,"user":{"displayName":"Cristina Ortega","userId":"10739822531022913396"},"user_tz":300},"id":"Pi096DV1bH0X","outputId":"d4e37502-fc7c-4d9d-876b-e5c1822244f7"},"outputs":[{"name":"stdout","output_type":"stream","text":["word_index = {'hola': 1, 'mundo': 2, 'a': 3, 'todos': 4, 'todo': 5, 'el': 6}\n","secuencias = [[1, 2], [1, 3, 4], [1, 3, 5, 6, 2]]\n","rellena = \n"," [[0 0 0 1 2]\n"," [0 0 1 3 4]\n"," [1 3 5 6 2]]\n"]}],"source":["import keras\n","\n","frases = [\n","    'Hola mundo',\n","    'Hola a todos',\n","    'Hola a todo el mundo'\n","]\n","\n","# Genera el diccionario de tokens\n","tokenizer = keras.preprocessing.text.Tokenizer(num_words=10)\n","tokenizer.fit_on_texts(frases)\n","word_index = tokenizer.word_index\n","print('word_index =', word_index)\n","\n","# Generación de secuencia tokenizadas\n","secuencias = tokenizer.texts_to_sequences(frases)\n","print('secuencias =', secuencias)\n","\n","# Rellena las secuencias a una longitud uniforme\n","relleno = keras.preprocessing.sequence.pad_sequences(secuencias)\n","print('rellena = \\n', relleno)"]},{"cell_type":"markdown","metadata":{"id":"2meAq79vHwTC"},"source":["Cuando el número de palabras en la frase es mayor al parámetro"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":262,"status":"ok","timestamp":1715387883696,"user":{"displayName":"Cristina Ortega","userId":"10739822531022913396"},"user_tz":300},"id":"WTQlv78YH4Rj","outputId":"7961554b-e109-4357-8148-a95bdac02322"},"outputs":[{"name":"stdout","output_type":"stream","text":["word_index = {'hola': 1, 'mundo': 2, 'a': 3, 'todos': 4, 'todo': 5, 'el': 6, 'buen': 7, 'dia': 8, 'como': 9, 'estas': 10, 'hoy': 11}\n","secuencias = [[1, 2], [1, 3, 4], [1, 3, 5, 6, 2], [7, 8, 9]]\n","rellena = \n"," [[0 0 0 1 2]\n"," [0 0 1 3 4]\n"," [1 3 5 6 2]\n"," [0 0 7 8 9]]\n"]}],"source":["quotes = [\n","    'Hola mundo',\n","    'Hola a todos',\n","    'Hola a todo el mundo',\n","    'Buen dia, como estas hoy'\n","]\n","\n","# Genera el diccionario de tokens\n","tokenizer = keras.preprocessing.text.Tokenizer(num_words=10)\n","tokenizer.fit_on_texts(quotes)\n","word_index = tokenizer.word_index\n","print('word_index =', word_index)\n","\n","# Generación de secuencia tokenizadas\n","sc = tokenizer.texts_to_sequences(quotes)\n","print('secuencias =', secuencias)\n","\n","# Rellena las secuencias a una longitud uniforme\n","fill = keras.preprocessing.sequence.pad_sequences(sc)\n","print('rellena = \\n', fill)"]},{"cell_type":"markdown","metadata":{"id":"IKoabui5Iomh"},"source":["Cuando el número de palabras en las frase es mayor al parámetro num_words ademas incluimos el parámetro OVV"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":213,"status":"ok","timestamp":1715387889644,"user":{"displayName":"Cristina Ortega","userId":"10739822531022913396"},"user_tz":300},"id":"xQQjvYXAI01I","outputId":"5aea8e47-fd78-4d45-cc7c-2965b2ce3c0d"},"outputs":[{"name":"stdout","output_type":"stream","text":["word_index = {'<OOV>': 1, 'hola': 2, 'mundo': 3, 'a': 4, 'todos': 5, 'todo': 6, 'el': 7, 'buen': 8, 'dia': 9, 'como': 10, 'estas': 11, 'hoy': 12}\n","secuencias = [[2, 3], [2, 4, 5], [2, 4, 6, 7, 3], [8, 9, 1, 1, 1]]\n","rellena = \n"," [[0 0 0 2 3]\n"," [0 0 2 4 5]\n"," [2 4 6 7 3]\n"," [8 9 1 1 1]]\n"]}],"source":["quotes = [\n","    'Hola mundo',\n","    'Hola a todos',\n","    'Hola a todo el mundo',\n","    'Buen dia, como estas hoy'\n","]\n","\n","# Genera el diccionario de tokens\n","tokenizer = keras.preprocessing.text.Tokenizer(num_words=10,\n","                                               oov_token=\"<OOV>\")\n","tokenizer.fit_on_texts(quotes)\n","word_index = tokenizer.word_index\n","print('word_index =', word_index)\n","\n","# Generación de secuencia tokenizadas\n","sc = tokenizer.texts_to_sequences(quotes)\n","print('secuencias =', sc)\n","\n","# Rellena las secuencias a una longitud uniforme\n","fill = keras.preprocessing.sequence.pad_sequences(sc)\n","print('rellena = \\n', fill)"]},{"cell_type":"markdown","metadata":{"id":"Sfyd6EUFKSbP"},"source":["Probemos los parámetros padding y truncating de la función pad_sequences en Keras se utilizan para controlar el relleno y el truncamiento de las secuencias derante el preprocesamiento de los datos"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":209,"status":"ok","timestamp":1715387895005,"user":{"displayName":"Cristina Ortega","userId":"10739822531022913396"},"user_tz":300},"id":"K8dQS49GKkp2","outputId":"f56d46d0-4757-4791-cb0b-1aebb58a6b76"},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","word_index = {'<OOV>': 1, 'hola': 2, 'mundo': 3, 'a': 4, 'todos': 5, 'todo': 6, 'el': 7, 'buen': 8, 'dia': 9, 'como': 10, 'estas': 11, 'hoy': 12}\n","secuencias = [[2, 3], [2, 4, 5], [2, 4, 6, 7, 3], [8, 9, 1, 1, 1]]\n","rellena = \n"," [[2 3 0 0 0]\n"," [2 4 5 0 0]\n"," [2 4 6 7 3]\n"," [8 9 1 1 1]]\n"]}],"source":["quotes = [\n","    'Hola mundo',\n","    'Hola a todos',\n","    'Hola a todo el mundo',\n","    'Buen dia, como estas hoy'\n","]\n","\n","# Genera el diccionario de tokens\n","tokenizer = keras.preprocessing.text.Tokenizer(num_words=10,\n","                                               oov_token = '<OOV>')\n","tokenizer.fit_on_texts(quotes)\n","word_index = tokenizer.word_index\n","print('\\nword_index =', word_index)\n","\n","# Generación de secuencia tokenizadas\n","sc = tokenizer.texts_to_sequences(quotes)\n","print('secuencias =', sc)\n","\n","# Rellena las secuencias a una longitud uniforme\n","fill = keras.preprocessing.sequence.pad_sequences(sc,\n","                                                     padding = 'post',\n","                                                     truncating = 'post')\n","print('rellena = \\n', fill)"]}],"metadata":{"colab":{"authorship_tag":"ABX9TyO+vLmX6IzHxTDTSxorftBI","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
